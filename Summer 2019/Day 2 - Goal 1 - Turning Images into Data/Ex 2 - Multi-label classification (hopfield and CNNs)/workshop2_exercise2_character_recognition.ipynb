{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "workshop2_exercise2_character-recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cgOhXAQeBu3",
        "colab_type": "text"
      },
      "source": [
        "# Handwritten Digit Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmX5eEBQeBu8",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Dataset description\n",
        "\n",
        "- The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. \n",
        "- It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. \n",
        "- Each digit has dimension 28x28 (i.e. 768) pixels, and is represented in grayscale.\n",
        "- There are only 10 digits (0 to 9) and classes to predict.\n",
        "- Results are reported using prediction error (i.e. inverted classification accuracy).\n",
        "- Excellent results have a prediction error of less than 1%, and superb results with errors of 0.2% can be achieved with large Convolutional Neural Networks\n",
        "\n",
        "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtvC9f9neBu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can use Keras to download the MNIST dataset for us! Let's see what\n",
        "# it looks like:\n",
        "%matplotlib inline\n",
        "\n",
        "# Plot ad hoc mnist instances\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# load (downloaded if needed) the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "# plot 4 images as gray scale\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XM61dYreBvG",
        "colab_type": "text"
      },
      "source": [
        "# Applying MNIST on a simple Neural Network (1 hidden layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrJ6JSGieBvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup\n",
        "import numpy\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# seed our random number for reproducibility\n",
        "seed = 314159\n",
        "random.seed(seed)\n",
        "\n",
        "# load (downloaded if needed) the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CHoGmRweBvL",
        "colab_type": "text"
      },
      "source": [
        "Since we're training a NN with pixels as input, we have to linearize / flatten our images. Hence, we'll be creating an input layer of 28x28 or 768 input neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL02WDWKeBvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten 28*28 images to a 784 vector for each image\n",
        "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
        "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7IwAcTheBvP",
        "colab_type": "text"
      },
      "source": [
        "Since these images are greyscale, **the pixel values are from 0 to 255**. As-is, we can apply this data on the NN, but it's a good idea to standardize the data by normalizing it. In this case, since it is well-known, we normalize by the maximimum possible pixel value, 255.\n",
        "\n",
        "*In theory, it's not necessary to normalize numeric x-data (also called independent data). However, practice has shown that when numeric x-data values are normalized, neural network training is often more efficient, which leads to a better predictor. Basically, if numeric data is not normalized, and the magnitudes of two predictors are far apart, then a change in the value of a neural network weight has far more relative influence on the x-value with larger magnitudes.*\n",
        "\n",
        "See: https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h86bq59KeBvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# we can verify that scaling has occurred:\n",
        "print(X_train.max())  # returns 1\n",
        "print(X_test.max())  # returns 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaNHS8xqeBvV",
        "colab_type": "text"
      },
      "source": [
        "Since our output variable will contain the numbers 0 - 9, this is a multi-class classification problem. So, it's a good idea to transform the class integers into a binary matrix (i.e. \"hot encoding\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYo_MVCieBvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg8y2nEeBva",
        "colab_type": "text"
      },
      "source": [
        "Now we define the model of the Neural Network (which can be adapted to improve the score):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix7j6TAkeBvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define baseline model\n",
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_pixels, \n",
        "                    input_dim=num_pixels, \n",
        "                    kernel_initializer='normal', \n",
        "                    activation='relu'))\n",
        "    model.add(Dense(num_classes, \n",
        "                    kernel_initializer='normal', \n",
        "                    activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4XB_0Y9eBve",
        "colab_type": "text"
      },
      "source": [
        "The model is a simple neural network with one hidden layer with the same number of neurons as there are inputs (784). A rectifier activation function is used for the neurons in the hidden layer.\n",
        "\n",
        "A softmax activation function is used on the output layer to turn the outputs into probability-like values and allow one class of the 10 to be selected as the modelâ€™s output prediction. Logarithmic loss is used as the loss function (called categorical_crossentropy in Keras) and the efficient ADAM gradient descent algorithm is used to learn the weights.\n",
        "\n",
        "We can now fit and evaluate the model. The model is fit over 10 epochs with updates every 200 images. The test data is used as the validation dataset, allowing you to see the skill of the model as it trains. A verbose value of 2 is used to reduce the output to one line for each training epoch.\n",
        "\n",
        "Now, we can use the test dataset and evaluate the model and print the classification error rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w594Y6ieBvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model = baseline_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYJTVFV1eBvk",
        "colab_type": "text"
      },
      "source": [
        "# Simple Convolutional Neural Network for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvSsOxXGeBvl",
        "colab_type": "text"
      },
      "source": [
        "Now that we have seen how to load the MNIST dataset and train a simple multi-layer perceptron model on it, it is time to develop a more sophisticated convolutional neural network or CNN model.\n",
        "\n",
        "Keras does provide a lot of capability for creating convolutional neural networks.\n",
        "\n",
        "In this section we will create a simple CNN for MNIST that demonstrates how to use all of the aspects of a modern CNN implementation, including Convolutional layers, Pooling layers and Dropout layers.\n",
        "\n",
        "The first step is to import the classes and functions needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9cMpgEZeBvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup imports and random number\n",
        "import numpy\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "# set a seed for reproducibility:\n",
        "seed = 314159\n",
        "random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTW9ASbZeBvp",
        "colab_type": "text"
      },
      "source": [
        "Next we need to load the MNIST dataset and reshape it so that it is suitable for use training a CNN. In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions `[pixels][width][height]`.\n",
        "\n",
        "In the case of RGB, the first dimension pixels would be 3 for the red, green and blue components and it would be like having 3 image inputs for every color image. In the case of MNIST where the pixel values are gray scale, the pixel dimension is set to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFlXI_EWeBvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcgCXeIzeBvt",
        "colab_type": "text"
      },
      "source": [
        "As before, it is a good idea to normalize the pixel values to the range 0 and 1 and one hot encode the output variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGJEPkKSeBvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBSTuhqieBvx",
        "colab_type": "text"
      },
      "source": [
        "Next we define our neural network model.\n",
        "\n",
        "Convolutional neural networks are more complex than standard multi-layer perceptrons, so we will start by using a simple structure to begin with that uses all of the elements for state of the art results. Below summarizes the network architecture.\n",
        "\n",
        "1. The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5Ã—5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above `[pixels][width][height]`.\n",
        "2. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2Ã—2.\n",
        "3. The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
        "4. Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
        "5. Next a fully connected layer with 128 neurons and rectifier activation function.\n",
        "6. Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.\n",
        "\n",
        "As before, the model is trained using logarithmic loss and the ADAM gradient descent algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHWQ_O17eBvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, \n",
        "                     (5, 5), \n",
        "                     input_shape=(1, 28, 28), \n",
        "                     activation='relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXC_19XheBv3",
        "colab_type": "text"
      },
      "source": [
        "Next, we evaluate the model the same way as before with the multi-layer perceptron. The CNN is fit over 10 epochs with a batch size of 200.\n",
        "\n",
        "The accuracy on the training and validation test is printed each epoch and at the end of the classification error rate is printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05oB1YrYj5ds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# go to Runtime and change runtime type and select GPU\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_IgZASAeBv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = baseline_model()\n",
        "\n",
        "# Fit the model using the GPU\n",
        "with tf.device('/gpu:0'):\n",
        "  model.fit(X_train, \n",
        "          y_train, \n",
        "          validation_data=(X_test, y_test), \n",
        "          epochs=10, \n",
        "          batch_size=200, \n",
        "          verbose=2)\n",
        "\n",
        "  # Final evaluation of the model\n",
        "  scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE-5U8SieBv8",
        "colab_type": "text"
      },
      "source": [
        "# Larger Convolutional Neural Network for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aooSL8c3eBv9",
        "colab_type": "text"
      },
      "source": [
        "Now that we have seen how to create a simple CNN, letâ€™s take a look at a model capable of close to state of the art results.\n",
        "\n",
        "We import classes and function then load and prepare the data the same as in the previous CNN example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLxSmo2oeBv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Larger CNN for the MNIST Dataset\n",
        "import numpy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 314159\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape to be [samples][pixels][width][height]\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXir0315eBwB",
        "colab_type": "text"
      },
      "source": [
        "This time we define a large CNN architecture with additional convolutional, max pooling layers and fully connected layers. The network topology can be summarized as follows.\n",
        "\n",
        "1. Convolutional layer with 30 feature maps of size 5Ã—5.\n",
        "2. Pooling layer taking the max over 2*2 patches.\n",
        "3. Convolutional layer with 15 feature maps of size 3Ã—3.\n",
        "4. Pooling layer taking the max over 2*2 patches.\n",
        "5. Dropout layer with a probability of 20%.\n",
        "6. Flatten layer.\n",
        "7. Fully connected layer with 128 neurons and rectifier activation.\n",
        "8. Fully connected layer with 50 neurons and rectifier activation.\n",
        "9. Output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kgiMXb8eBwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the larger model\n",
        "def larger_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(30, \n",
        "                     (5, 5), \n",
        "                     input_shape=(1, 28, 28), \n",
        "                     activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam', \n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFHbGV1ZeBwM",
        "colab_type": "text"
      },
      "source": [
        "Like the previous two experiments, the model is fit over 10 epochs with a batch size of 200.\n",
        "\n",
        "Running the example prints accuracy on the training and validation datasets each epoch and a final classification error rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmKH7Vc6eBwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model\n",
        "model = larger_model()\n",
        "\n",
        "# Fit the model using the available GPU\n",
        "with tf.device('/gpu:0'):\n",
        "  model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "\n",
        "  # Final evaluation of the model\n",
        "  scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhYlVxvVeBwW",
        "colab_type": "text"
      },
      "source": [
        "This slightly larger model takes a bit longer per epoch than the previous, and achieves the respectable classification error rate of 0.76%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv1PRCwWeBwX",
        "colab_type": "text"
      },
      "source": [
        "# Activity 1: How does the classification error change if you train the NN using the unnormalized data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzH_3rs_eBwY",
        "colab_type": "text"
      },
      "source": [
        "# Activity 2: How does the choice of activation function affect the NN and its classification error rate?\n",
        "\n",
        "Hint: Keras supports the following activation functions: `softplus`, `softsign`, `softsign`, `relu`, `tanh`, `sigmoid`, `hard_sigmoid`, and `linear`. See: https://keras.io/activations/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mtx-JApeBwZ",
        "colab_type": "text"
      },
      "source": [
        "# Activity 3: Randomly choose some test data, evaluate it using the NN, and visualize their classifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM-VLIjoeBwd",
        "colab_type": "text"
      },
      "source": [
        "# Reference: \n",
        "1. https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/"
      ]
    }
  ]
}