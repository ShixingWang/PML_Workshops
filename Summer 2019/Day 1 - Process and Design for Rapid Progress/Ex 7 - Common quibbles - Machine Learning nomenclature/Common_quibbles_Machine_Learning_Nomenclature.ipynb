{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Common_quibbles_Machine_Learning_Nomenclature.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTsxiE3ySdO5",
        "colab_type": "text"
      },
      "source": [
        "# Common Quibbles\n",
        "\n",
        "> \"Where does machine learning end and AI begin?\"\n",
        "> \n",
        "> \"Isn't machine learning just statistical inference?\"\n",
        "> \n",
        "> \"Is deeplearning machine learning or AI?\"\n",
        "> \n",
        "> \"I don't do data science, I do real science!... don't I?\"\n",
        "> \n",
        "> \"That isn't \\__\\__\\__\\__\\, stop calling it \\__\\__\\__\\__\\!\"\n",
        "> \n",
        "> \"Brains don't compute, stop talking about 'neural code'.\"\n",
        "> \n",
        "> \"Everything can be a computer the whole world is just information.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Here I try to sort out the different perspectives you are likely to encounter and help you avoid snipes.\n",
        "\n",
        "You've used a word and abused it's definition, but anyone can figure out what you mean by context. Only now, instead of considering your idea, your conversation partner is focusing soley on that one little thing. It's not clear if they'll ever take your idea seriously now. They seem to think it indicates the whole line of reasoning is corrupt... You've been sniped. \n",
        "\n",
        "Many of the people expressing interest in our workshops communicated that nailing down the meanings of the concepts above would be an important goal to focus on. Knowing how to use the right vocabulary can not only help us keep conversations going, it helps us Google Search for answers to our questions. \n",
        "\n",
        "Unfortunately, in a fast chaning field few concepts have fixed definitions, and anyone claiming that they can make it crystal clear for you is either a quack or about to seriously qualify those claims (like I just did). The best we can do is point out how differences and similarities in the way different fields use certain words. This is much more helpful than claiming to \"nail it down\" to one thing.\n",
        "\n",
        "If your interest in the vocabulary of computational data analysis is so you can settle a personal bet or dispute, then please reconsider. Argue from definitions, not about definitions. If you think the definition of a word is the thing that will win an argument, then you have a weak argument. Ask the other person to define the word for you, then reconsider their assertions *as if* that definition were true (even when you seriously disagree). There is a literal infinity of concepts which no language has any word for. People have a tendancy to grab the word which closely approximates their notions, accept that and work with them to understand. What's more, a valuable insight can come from accidents, meaning they can come from anywhere, even a mind that doesn't *yet* know what the words mean. So go along with poor word choice, after all science is about collaboration and dictionaries win fights, not disagreements.\n",
        "\n",
        "If your interest in the vocabulary is to have peace of mind and clarity about what research communities will have insights you can use, and to facilitate your communication to the broadest possible audience, then you're on the right track. Using proper terminology is important and should be everyones goal. Just remember to be patient with people. Different communities use terminology differently. Researchers of different ages will use terminology differently too as language changes over time, but they might have missed it either because they are inexperienced and have only read \"the classics\", or because they are very experienced and don't view the \"classic debates\" as settled. \n",
        "\n",
        "In what follows I try to proceed with basic terminology and at the end I include specific terms which can help you in your google searches. It is free to edit so anyone can contribute terminology. \n",
        "\n",
        "\n",
        "\n",
        "Pat Connely's Tavern in dog town, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGwL6baSUCuH",
        "colab_type": "text"
      },
      "source": [
        "## Computational Data Analysis vs Data Science vs Real Science... What do we call what we are doing?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### What are you doing here?\n",
        "If you're here, then you have a project in mind or want to learn some skills. You are probably a scientist or other researcher. You may have a healthy skepticism of buzzwords and the desire to distance yourself from them. \n",
        "As a scientist or other researcher, you need to analyze some data. If you use a computer to do that then you are doing \"computational data analysis\", this is often shortened to just \"data analysis\". \n",
        "\n",
        "### What are other people doing with the same tools?\n",
        "\n",
        "A lot different people can be doing computational data analysis for a lot of different reasons. I've done it to create schedules for my wife (a special education teacher) and to make predictive inventory systems to cut down on my shopping trips. Other people can do it for business or policy reasons. \n",
        "\n",
        "However, as a scientist, you are using your data to update some kind of model. A model is anything which you can manipulate or think about and thereby learn something about the object you intend to model. Not all data analysis has a model. Sometimes people just have a question and want an answer (e.g. How popular is the president). As a scientist all of your questions are designed to improve or test models, even if it is as simple as a conceptual model that usually goes un-articulated. A data scientist is in a special situation, they may be trying to answer questions for some one else and that other person might not have a model. If all that takes to answer the question is some straightforward data gathering and plot making then typically we call that \"data analysis\". A data scientist is hired when the questions are complicated enough that a model needs to be created in order to answer it. Sometimes the model is not a model of a real-world concept (like in science), but rather a \"statistical model\" which is a set of rules (classically mathematical functions) that improve the accuracy of predictions or inference. \n",
        "\n",
        "There are two other words that you might encounter: Informatics and \"quant\". \"Quant\" is a noun, it was coined before data science entered mainstream usage but applies to a data scientist working on wall street to predict the behavior of investements and to create new investment products. They don't necessarily use the same tools. Quants have historically used more pure mathematics. \"Informatics\" is the same thing. According to the [American Health Information Management Association](http://bok.ahima.org/doc?oid=302313#.XTDZ3uhKiUk): \"AHIMA defines informatics as a collaborative activity that involves people, processes, and technologies to produce and use trusted data for better decision making.\" This is data science. Trying to define these as separate things is just splitting hairs. In reality it's the same idea that emerged independently in different places: Evidence based practices and decisions give you a competitive edge and by analyzing and interpreting lots of data you can gather more evidence to base practices and decisions on.  In any case, you'll need to be making and updating conceptual and statistical models and that sounds an awful lot like science. \n",
        "\n",
        "####Examples:\n",
        "Data analysis: https://www.realclearpolitics.com/epolls/other/president_trump_job_approval-6179.html\n",
        "\n",
        "Reason: They just pick polls and then do an average, it's straightforward plot generation with a small amount of specialized knowledge. \n",
        "\n",
        "Simple \"data science\": https://projects.fivethirtyeight.com/trump-approval-ratings/\n",
        "\n",
        "Reason: They calculate \"house effects\" and rate polls in a sophisticated way then combine them accordingly. Thus they have a statistical model. Furthermore they carryout comparisons to other data sets, share uncertainty information, and try to characterize the national feeling, rather than just average polls and let the reader do the interpretation.\n",
        "\n",
        "Advanced \"data science\": https://www.wired.com/2009/09/how-the-netflix-prize-was-won/\n",
        "Reason: They build a complex statistical model of human beings, the consumer is modelled. The modeling is carried out by identifying statistical patterns. So it is both a statistical model, and a model of a human beings decision making process. \n",
        "\n",
        "Real science and data science: https://www.cell.com/cell/fulltext/S0092-8674(19)30391-5\n",
        "Reason: They used a statistical model, learned on animals to update a conceptual model held in the minds of many neuroscientists which is commonly referred to as \"neural code\". \"Neural code\" is a model, its an idea and we think about it to get inspiration for future experiments which then uncover facts. Rather than being a real thing, it refers to a real thing.\n",
        "\n",
        "\n",
        "<img src=https://4.bp.blogspot.com/-0cbXveb1J_0/V-FtjJZ4rqI/AAAAAAAAMHM/bS32Pio2a1IFOyp5T86S0jiyB-3KAN1iwCEw/s320/download%2B%25281%2529.png width=400><img src=https://4.bp.blogspot.com/-3shyHBawRZ8/V-Fxoul_KaI/AAAAAAAAMHo/zPKZfNZFrk08Zyuviz8pAqyJZSRpJr3OwCLcB/s400/DataScienceDisciplines.png width=500>\n",
        "\n",
        "\n",
        "\n",
        "https://www.kdnuggets.com/2016/10/battle-data-science-venn-diagrams.html/2\n",
        "\n",
        "https://www.sisense.com/blog/data-science-vs-data-analytics/\n",
        "\n",
        "https://www.northeastern.edu/graduate/blog/data-analytics-vs-data-science/\n",
        "\n",
        "https://www.simplilearn.com/data-science-vs-big-data-vs-data-analytics-article\n",
        "\n",
        "http://www.rayidghani.com/is-data-science-a-real-science\n",
        "\n",
        "https://towardsdatascience.com/is-data-science-a-real-science-2920bb2529aa\n",
        "\n",
        "Science and data science https://www.pnas.org/content/114/33/8689\n",
        "\n",
        "https://www.kdnuggets.com/2019/05/data-science-vs-decision-science.html\n",
        "\n",
        "https://medium.com/@dema300w/data-science-vs-actual-science-9a8b1b6c5ac4\n",
        "\n",
        "https://towardsdatascience.com/computational-biology-fca101e20412\n",
        "\n",
        "https://coursera.community/data-science-8/data-science-versus-bioinformatics-what-are-the-differences-173\n",
        "\n",
        "http://informaticsprofessor.blogspot.com/2015/07/what-is-difference-if-any-between.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0nlKMqTffT9",
        "colab_type": "text"
      },
      "source": [
        "## Machine Learning vs statistical inference\n",
        "\n",
        "I've had plenty of conversations about whether Machine Learning is just Statistical Inference with a computer. Often the context is that machine learning \"isn't really learning\". That posits that learning in the human brain isn't statistical inference. The human brain certainly doesn't adhere completely to any statistical inference model we have, but it does carry a lot of signatures that indicate statistical patterns underly our perception and decisions. So the distinction between \"true learning\" and \"statistical processes\" isn't clear. Nonetheless it's important to understand how these concepts are similar and different so that you can communicate clearly. \n",
        "\n",
        "It's best to start by introducing the terms: \"inference\" and \"prediction\". \"Inference\" is akin to measurement. A measurement is a direct application of a tool to standardize a description of a thing or system. Inference is the application of a more complicated procedure to describe something that you cannot directly observe or measure. Often the goal of inference is to indirectly make a measurement, but it can also be used to test a hypothesis or uncover a qualitative detail. A prediction is kind of inference. One can predict the outcome of an experiment or the future state of a system. Predictions do not have to be about events, but they are all associated with probabilities. An inference is not necessarily associated with a probabilty, unless it is a \"statistical inference\". \n",
        "\n",
        "Here is where Machine Learning and Statistical Inference are similar, both are probabalistic predictions about a feature of interest. They differ, both by their history and name and by the basic quantities which they consider. \n",
        "\n",
        "Statistical Inference deals explicity with variability and treating features as random variables. With statitistical inference you are explicitly manipulating probability distributions, such as considering whether the signature of an event is sufficiently above the level of noise in the experiment. This means you provide the data and the model and draw inference based on knowledge of the model. \n",
        "\n",
        "Machine Learning deals with probability distributions *implicitly*. The model parameters are often internal and not examined with mathematical analysis. Where as statistical inference already has a model, a machine learning algorithm creates it's own model. This doesn't happen entirely from scratch though. the algorithm starts with an essential structure. Decistion trees are series of pass fail criterion which nest or cascade. The Decisition Tree algorithm learns how many criterion to use and what those criterion are, probability distributions are not explicity a part of the model. Naive Bayes is a lengthy conditional probability statement, (e.g. what's the probability something is a SPAM email given that it contains certain words), the algorithm identifies features and creates the probability distributions. In each case the accuracy of the model, expressed with probability, is entirely empiracle. There is no simple function which gives us a confidence interval. If we have a confidence interval, it's because we tested the model on some examples. \n",
        "\n",
        "\n",
        "\n",
        "https://en.wikipedia.org/wiki/Timeline_of_machine_learning\n",
        "\n",
        "https://healthcare.ai/machine-learning-versus-statistics-use/\n",
        "\n",
        "https://www.kdnuggets.com/2016/11/machine-learning-vs-statistics.html\n",
        "\n",
        "Statistics versus machine learning https://www.nature.com/articles/nmeth.4642\n",
        "\n",
        "https://towardsdatascience.com/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyA8QKhCfqzL",
        "colab_type": "text"
      },
      "source": [
        "## Machine Learning vs Deep learning vs AI\n",
        "\n",
        "This is perhaps the simplest set of terminology to set straight. These are simply nested terms. Everyone generally agrees that Deep Learning is a subset of Machine Learning, and it is also a subset of AI. However some disagreement exists as to whether AI is a subset of machine learning, or whether Machine Learning is a Subset of AI. Unsurprisingly it's context that makes the difference. Almost every academic source you will find puts Machine Learning inside of Artificial Intelligence. \n",
        "\n",
        "This is because AI is the creation of a machine that can make decisions, and because AI is much older. A human being can impart their wisdom in a ruleset and then the machine can use that to make decisions without learning anything. However learning requires deciding what information is relevant to the desired outcome and what is not. A Machine Learning algorithm modifies itself, therefore it should fit within AI. If the person you are talking too is mostly interested in the results of the algorithm they may not consider Machine Learning to be a part of AI. So when talking to non-academics you may find that they associate AI with machines that take autonomously take actions. So they would resist calling a classifier, clustering algorithm, or a statistical model, an AI because they merely guide humans to the right actions to take. If you want to read about Machine Learning research, you better still look in AI journals and programs. Deep learning is simply the composition of multiple Machine Learning algorithms. Often this involves the creation of new algorithms to facilitate linking them together but Deep Learning fits squarely inside of Machine Learning, and since reinforcement learning (learning actions) is a popular topic within deep learning, it fits into AI no matter how you look at it. \n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/jojker/PML_Workshops/master/Summer%202019/Day%201%20-%20Process%20and%20Design%20for%20Rapid%20Progress/Ex%207%20-%20Common%20quibbles%20-%20Machine%20Learning%20nomenclature/Resources/Academic%20usage%20AI%20ML%20and%20DL.png\" width=400><img src=\"https://raw.githubusercontent.com/jojker/PML_Workshops/master/Summer%202019/Day%201%20-%20Process%20and%20Design%20for%20Rapid%20Progress/Ex%207%20-%20Common%20quibbles%20-%20Machine%20Learning%20nomenclature/Resources/Popular%20usage%20AI%20ML%20and%20DL.png\" width=400>\n",
        "\n",
        "\n",
        "https://skymind.ai/wiki/ai-vs-machine-learning-vs-deep-learning\n",
        "\n",
        "https://medium.com/datadriveninvestor/differences-between-ai-and-machine-learning-and-why-it-matters-1255b182fc6\n",
        "\n",
        "https://fortune.com/2019/05/28/ai-buzzwords/amp/\n",
        "\n",
        "https://www.technologyreview.com/s/612437/what-is-machine-learning-we-drew-you-another-flowchart/\n",
        "\n",
        "https://towardsdatascience.com/clarity-around-ai-language-2dc16fdb6e82"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzDswnx4fvTD",
        "colab_type": "text"
      },
      "source": [
        "## Computer Vision vs all of the above\n",
        "\n",
        "Computer vision, is AI, but is not necessarily Machine Learning. It's also important to separate it from image processing. Computer vision is the extraction of information from images, often through methods that are bestowed upon machines directly by human experience. This means that human worked out the mathematics of something and implemented it (e.g. edge detection). This dependence on human experience for know what information you can extract and how may have limited computer vision. Nowadays people are exploring machine learning and deeplearning to extract surprising information, such as seeing around corners by reconstructing images from the patterns of shadows and reflections on the floor. \n",
        "\n",
        "Image processing is the enhancement of images. It's often a first step for machine learning or computer vision algorithms but is not generally considered computer vision. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/computer-vision-concepts-and-terminology-edd392a6f594\n",
        "\n",
        "https://en.wikipedia.org/wiki/Glossary_of_machine_vision\n",
        "\n",
        "https://www.visiononline.org/market-data.cfm?id=73\n",
        "\n",
        "https://www.rsipvision.com/defining-borders/\n",
        "\n",
        "https://www.analyticsindiamag.com/what-is-the-difference-between-computer-vision-and-image-processing/\n",
        "\n",
        "https://www.quora.com/What-is-the-difference-between-image-processing-and-computer-vision\n",
        "\n",
        "https://naadispeaks.wordpress.com/2018/08/12/deep-learning-vs-traditional-computer-vision/\n",
        "\n",
        "Deep Learning vs. Traditional Computer Vision https://link.springer.com/chapter/10.1007/978-3-030-17795-9_10\n",
        "\n",
        "https://zbigatron.com/has-deep-learning-superseded-traditional-computer-vision-techniques/\n",
        "\n",
        "https://www.quora.com/What-is-the-relationship-between-computer-vision-machine-learning-artificial-intelligence-and-deep-learning-Are-they-dependent-and-tightly-linked\n",
        "\n",
        "https://medium.com/@dataturks/deep-learning-and-computer-vision-from-basic-implementation-to-efficient-methods-3ca994d50e90"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0aMl7sxf3s7",
        "colab_type": "text"
      },
      "source": [
        "## Control theory vs AI\n",
        "\n",
        "If you've been thinking about what intelligence is and how machines that might pass a Turing test might be constructed or what avenues of research might lead there, and you haven't looked seriously at control theory then you've been missing an enormous part of the equation. \n",
        "\n",
        "Control theory is a mathematical approach to controlling systems. It's often thought of in terms of thermostats, or controlling the print-nozzle of a 3D printer or scanning head of a Scanning Electron Microscope. However, a mid-century pioneer was Norbert Weiner, an odd-duck who invented a field he called \"cybernetics\" which he described as \"the scientific study of control and communication in the animal and the machine.\" Cybernetics was deeply routed in the dynamical systems perspective inherited from control theory, and was a leading contender for the title of \"most likely to result in an intelligent machine\". Isaac Asimov's \"Positronic Brain\" (and hence Commander Data from Star Trek) was a riff off the ideas of cybernetics. \n",
        "\n",
        "Since control theory is all about controlling a device it's application has a lot of overlap with the portion of Artificial Intelligence devoted to physical action, especially reinforcment learning. Reinforcement learning is about learning what actions to take, given a machines current state, the state of all the relevent agents in the environment, and the desired outcome. As such it's one approach to creating a control system for a complex device, as well as a system that plays a game (such as Go). \n",
        "\n",
        "There are other ways to implement control systems using AI, but today ideas from control theory rarely enter Machine Learning systems. Control theory still plays a prominant role in the neuroscience of motor systems so it may yet make an incursion but for mainstream software packages you will be able to apply in your research, they are mostly separate. \n",
        "\n",
        "\n",
        "https://www.quora.com/What-is-the-difference-between-optimal-control-theory-and-reinforcement-learning\n",
        "\n",
        "https://towardsdatascience.com/reinforcement-learning-motivation-for-a-control-engineer-52ecc181ec25\n",
        "\n",
        "https://en.wikipedia.org/wiki/Machine_learning_control\n",
        "\n",
        "https://news.ycombinator.com/item?id=11452335\n",
        "\n",
        "https://ai.stackexchange.com/questions/11375/what-is-the-difference-between-reinforcement-learning-and-optimal-control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekhn7O8K_jyX",
        "colab_type": "text"
      },
      "source": [
        "## Do brains \"compute\"?\n",
        "\n",
        "The word \"compute\" is a classic example of a word whose meaning has changed over time, AND a word which is widely mis-understood even by very smart people. \"Information\" is a word with a similar affliction. \n",
        "\n",
        "There is a lot of debate about whether brains compute, whether information theory is applicable to neuroscience, or whether \"neural code\" makes sense as concept. For each side, if you posit the definitions they hold, then each side is correct. Those who would like to say the brain does not compute have a defintion of \"compute\" which is so narrow that the brain obviously does not compute. Those who would like to say it does have a defintion of \"compute\" that is so general anything could be a computer. If you want some evidence to decide whether one perspective is more useful than the other, then I invite you to consider the field of \"reservoir computing\". This field involves research in how to perform \"computations\" with any dynamical object, no matter how unstructured it is, with only a minimal structured interface. This field has had much realworld success proving the principle that anything can be a computer... \n",
        "\n",
        "**The old defintion of compute:** \n",
        "\n",
        "To determine by arithmetical or mathematical reckoning; to calculate, reckon, count. In later use chiefly: to ascertain by a relatively complex calculation or procedure, typically using a computer or calculating machine. (Oxford English Dictionary ~ not especially relevant)\n",
        "\n",
        "**The real old defintion of compute:** \n",
        "\n",
        "In reality, the debate began with the recognition that the action potentials (or \"spikes) of neurons could be thought of as singlar events. Spikes are so unique that [maybe] all other neural responses could be ignored and the brain could be treated as fundamentally *digital*. In otherwords you could represent the *state* of a piece of brain tissue entirely by ones and zeros. A neuron would be labeled with a zero if it was not currently spiking, and one otherwise. This sounds a lot like the way silicon computers store and process information. Researchers began looking for logic gates in the brain, and a lot of early work in artificial neural networks demonstrated the power of their methods by demonstrating the ability to perform boolean logic. So **ONE** of the old definitions of \"compute\" is \"to process digital information with interconnected boolean operators\". This is a pretty neive approach, both in the inspiration and result. As computer science matured more careful researchers began to try to describe the brain in terms of input-output relationships. They were optimistic that by doing so they could develop theories about the black-box neural-junk in between and test those theories by examining connectivity. This is reminiscent of Skinnerian Behaviorism, and lead to a **more astute definition**, \"to carry out an algorithm with a deterministic finite automata\". This is a very good definition because it directly invokes the definition of \"computability\". An input-output relationship (AKA \"function\") is \"computable\" if it is capable of being computed by any deterministic algorithm in any finite amount of time. \n",
        "\n",
        "**The correct objections to this idea of computation:**\n",
        "\n",
        "Researchers have rightly pointed out that spiking is highly variable from one trial to the next, and that things like \"volume transmission\" and dendro-dendritic synapses all have non-negligable impacts on brain function and organism behavior. What's more neurons are not strictly connected in a feedforward architecture and those connections can change! They are constantly in flux. If a theory defines inputs and outputs according to which neurons spiked at what precise times then that theory cannot describe the brain very well. \n",
        "\n",
        "This is true, the old definitions of \"compute\" are not especially useful for thinking about the brain. \n",
        "\n",
        "**The real definition of compute:**\n",
        "\n",
        "Modern computer scientists are comfortable with the concept of the stochastic or asynchronous cellular automata. Automata theory is an expansive mental framework for investigating machines and mechanisms. Deterministic finite automata is the subset that describes your home computer. Cellular automata is a different branch of automata theory that describes machines doing just the kind of thing *parts* the brain do. Parallelism, constant rewiring, multiple forms of communication, and context dependent encodings are all OK in this paradigm. In this paradigm the brain would be described by a collection of cellular automata. In otherwords a recurrently connected computational graph with a clustered architecture across multiple scales and where there are multiple types of interactions. In this context to compute means \"Any input-output operation describable with any part of automata theory\". This is a direct acknowledgement that many properties of the outputs of certain kinds of cellular automata are non-computable, even though they are simulated with a digital silicon computer.  \n",
        "\n",
        "Despite the likely non-computability of brain behavior, computer science offers a tractable method of analysis. (See: [Computation Theory of Cellular Automata](https://www.stephenwolfram.com/publications/academic/computation-theory-cellular-automata.pdf)) Stephen Wolfram, in his book \"[A New Kind of Science](https://www.wolframscience.com/nks/)\" argues that automata theory represents an alternative to classic mathematics that can also describe the entire natural world.\n",
        "\n",
        "**A word about logic gates:**\n",
        "An important recurring theme, in neuroscience and computer science is the role of logic gates. Logic gates are units which carry out a boolean operation: OR, AND, NOT, XOR. Boolean operators act on binary true/false or zero/one inputs and outputs. However, similar operations can be described for continous variables and \"logic gates\" is the most approximate terminology. Considering real numbers between zero and one, *a* and *b*, the operation (*a*+*b*)/2 is an OR-like operation. While *a*x*b* is an AND-like operation, and *a*x(1-*b*)+(1-*a*)x*b* is an XOR like operation. You may encounter researchers describing logical circuits implimented with biological processes, be it dendritic \"computation\" or slime-mold \"bio-computers\". Typically they are referring to a thresholding operation applied to this kind of continuous function. Often they do mean to suggest a network of boolean operations, but other times they simple need a vocabulary to make an efficient exposition.\n",
        " \n",
        "\n",
        "https://en.wikipedia.org/wiki/Behaviorism\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0893608019300784\n",
        "\n",
        "https://medium.com/@stevenyue/cellular-automata-reservoir-computing-and-beyond-8cac22340449\n",
        "\n",
        "https://phys.org/news/2015-05-scientists-atomic-scale-hardware-natural.html\n",
        "\n",
        "https://en.wikipedia.org/wiki/Reservoir_computing\n",
        "\n",
        "https://phys.org/news/2016-10-self-learning-tackles-problems-previous.html\n",
        "\n",
        "https://medium.com/the-spike/brains-as-analog-computers-fa297021f935\n",
        "\n",
        "https://www.forbes.com/sites/alexknapp/2012/05/04/why-your-brain-isnt-a-computer/#3800d10f13e1\n",
        "\n",
        "https://www.jstor.org/stable/3130074?seq=1#metadata_info_tab_contents\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0149763497000213"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbPgVLwagGDK",
        "colab_type": "text"
      },
      "source": [
        "## Projects and Vocabulary\n",
        "https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_zgYXY-TPGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}